{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0199a367",
   "metadata": {},
   "source": [
    " # 8주차 실습 -  featureset - bagofwords, tfidf, countvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a75200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\박승연\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\박승연\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\박승연\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텍스트 전처리에 필요한 라이브러리 불러오기\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffdf7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#분석할 corpus 가져오기\n",
    "\n",
    "corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n",
    "\t\t\"Look at it at least twice and definitely watch part 2.\",\n",
    "\t\t\"It will change your view of the matrix\",\n",
    "\t\t\"Are the human people the ones who started the war?\",\n",
    "\t\t\"Is AI a bad thing?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fce8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이썬에서 정의된 stopwords와 lemmatizer 불러오기\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b4b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 전처리 과정을 담은 tokenize 라는 함수를 정의\n",
    "\n",
    "def tokenize(text):\n",
    "   \n",
    "   # normalize case and remove punctuation\n",
    "   text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "   \n",
    "   # tokenize text\n",
    "   tokens = word_tokenize(text)\n",
    "   \n",
    "   # lemmatize and remove stop words\n",
    "   tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "   \n",
    "   return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7710174",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc9e8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 단어들을 벡터로 변형\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer = tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc8f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터의 각 단어 빈도를 구하기\n",
    "X = vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50cfccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#제 이 Sparse matrix를 배열(array)로 변환\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "514280e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 6,\n",
       " 'time': 20,\n",
       " 'see': 17,\n",
       " 'second': 16,\n",
       " 'renaissance': 15,\n",
       " 'may': 11,\n",
       " 'look': 9,\n",
       " 'boring': 3,\n",
       " 'least': 8,\n",
       " 'twice': 21,\n",
       " 'definitely': 5,\n",
       " 'watch': 24,\n",
       " 'part': 13,\n",
       " '2': 0,\n",
       " 'change': 4,\n",
       " 'view': 22,\n",
       " 'matrix': 10,\n",
       " 'human': 7,\n",
       " 'people': 14,\n",
       " 'one': 12,\n",
       " 'started': 18,\n",
       " 'war': 23,\n",
       " 'ai': 1,\n",
       " 'bad': 2,\n",
       " 'thing': 19}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#각 단어들을 벡터로 변형\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a996a",
   "metadata": {},
   "source": [
    "## TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d958c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#tf-idf 초기화\n",
    "transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69582107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카운트 벡터라이저 결과의 카운트를 사용하여 tf-idf 값 계산\n",
    "tfidf = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b284333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.36419547, 0.        ,\n",
       "        0.        , 0.36419547, 0.        , 0.        , 0.26745392,\n",
       "        0.        , 0.36419547, 0.        , 0.        , 0.        ,\n",
       "        0.36419547, 0.36419547, 0.36419547, 0.        , 0.        ,\n",
       "        0.36419547, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.39105193, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39105193, 0.        , 0.        , 0.39105193, 0.28717648,\n",
       "        0.        , 0.        , 0.        , 0.39105193, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.39105193, 0.        , 0.        , 0.39105193],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.4472136 ,\n",
       "        0.        , 0.        , 0.        , 0.4472136 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 배열로 변환\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b35f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tf-idf 초기화\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f214764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of word 와 tf-idf 값 계산\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca6d699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30298183, 0.        , 0.        , 0.30298183, 0.        ,\n",
       "        0.        , 0.20291046, 0.        , 0.24444384, 0.        ,\n",
       "        0.30298183, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30298183, 0.30298183, 0.30298183, 0.        , 0.40582093,\n",
       "        0.        , 0.30298183, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30298183, 0.        ],\n",
       "       [0.        , 0.30015782, 0.        , 0.60031564, 0.        ,\n",
       "        0.        , 0.        , 0.30015782, 0.        , 0.        ,\n",
       "        0.        , 0.20101919, 0.30015782, 0.24216544, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30015782, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30015782, 0.        , 0.        ,\n",
       "        0.30015782, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.38077552, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25500981, 0.        , 0.        , 0.38077552,\n",
       "        0.        , 0.38077552, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25500981,\n",
       "        0.        , 0.        , 0.        , 0.38077552, 0.        ,\n",
       "        0.        , 0.        , 0.38077552, 0.        , 0.38077552],\n",
       "       [0.        , 0.        , 0.30101067, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30101067,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30101067, 0.        , 0.30101067,\n",
       "        0.        , 0.        , 0.        , 0.30101067, 0.60477106,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30101067,\n",
       "        0.        , 0.30101067, 0.        , 0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 배열로 변환\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6bb09f",
   "metadata": {},
   "source": [
    "# 9주차 실습 - 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832a1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5328abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "mpl.rc('font', family='NanumGothic') # 폰트 설정\n",
    "mpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정\n",
    "\n",
    "# 차트 스타일 설정\n",
    "sns.set(font=\"NanumGothic\", rc={\"axes.unicode_minus\":False}, style='darkgrid')\n",
    "plt.rc(\"figure\", figsize=(10,8))\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b3d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 8개 주제 추출\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
    "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med'  ]\n",
    "\n",
    "# 위에서 지정한 주제만 추출\n",
    "news_df= fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'), \n",
    "                            categories=cats, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f705a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Shape: (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 사이킷런 내부 데이터 20 뉴스그룹 데이터 셋에서 일부 주제의 기사만 추출\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LDA는 CountVectorizer만 적용\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "\n",
    "print('CountVectorizer Shape:', feat_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d958d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_df로 빈도수가 95% 이내인 단어만 추출한다.\n",
    "\n",
    "#max_features로 피처의 갯수는 가장 높은 빈도를 가진 단어 순으로 1,000개 사용\n",
    "\n",
    "#min_df로 빈도수가 2 이하인 단어는 제외\n",
    "\n",
    "#LDA에서 피처 벡터화는 CountVectorizer만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16e9dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=8, random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e296568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
       "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
       "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
       "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
       "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
       "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
       "       ...,\n",
       "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
       "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
       "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
       "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
       "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
       "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LDA에서 n_components로 토픽의 갯수를 조정\n",
    "\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "101deead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da99947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 피처명 추출 함수\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_news20 = [\"모토사이클\", \"야구\", \"그래픽스\", \"윈도우즈\", \"중동\", \"기독교\", \"전자공학\", \"의학\"]\n",
    "    \n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index + 1, topic_news20[topic_index])\n",
    "\n",
    "        # 각 토픽별 word 피처 연관도 내림차순 정렬시 값들의 index 반환 .. (1)\n",
    "        topic_word_indexes = topic.argsort()[::-1] # [::-1] 역순으로 정렬\n",
    "        top_indexes = topic_word_indexes[:no_top_words]\n",
    "        \n",
    "        # (1)의 index로 피처 이름명 추출\n",
    "        feature_concat = '/'.join([feature_names[i] for i in top_indexes])                \n",
    "        \n",
    "        print(feature_concat)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a24eacc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 1 모토사이클\n",
      "year/10/game/medical/health/team/12/20/disease/cancer/1993/games/years/patients/good\n",
      " \n",
      "Topic # 2 야구\n",
      "don/just/like/know/people/said/think/time/ve/didn/right/going/say/ll/way\n",
      " \n",
      "Topic # 3 그래픽스\n",
      "image/file/jpeg/program/gif/images/output/format/files/color/entry/00/use/bit/03\n",
      " \n",
      "Topic # 4 윈도우즈\n",
      "like/know/don/think/use/does/just/good/time/book/read/information/people/used/post\n",
      " \n",
      "Topic # 5 중동\n",
      "armenian/israel/armenians/jews/turkish/people/israeli/jewish/government/war/dos dos/turkey/arab/armenia/000\n",
      " \n",
      "Topic # 6 기독교\n",
      "edu/com/available/graphics/ftp/data/pub/motif/mail/widget/software/mit/information/version/sun\n",
      " \n",
      "Topic # 7 전자공학\n",
      "god/people/jesus/church/believe/christ/does/christian/say/think/christians/bible/faith/sin/life\n",
      " \n",
      "Topic # 8 의학\n",
      "use/dos/thanks/windows/using/window/does/display/help/like/problem/server/need/know/run\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer 객체의 전체 word 명칭\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# Topic별 가장 연관도가 높은 word 15개\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e221165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#벡터 객체의 get_feature_names()을 이용해 전체 word 피처 명칭을 알 수 있다.\n",
    "\n",
    "#각 토픽별로 연관성이 높은 단어를 봤을 때 그래픽스는 image, jpeg등 관련 주제어가 잘 추출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12701e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
